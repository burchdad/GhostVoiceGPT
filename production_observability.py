"""
Production Observability & QA Test Suite
Comprehensive monitoring, testing, and quality assurance for voice calls
"""

import asyncio
import time
import json
import statistics
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import logging
import random

class CallQuality(Enum):
    """Call quality ratings"""
    EXCELLENT = 5
    GOOD = 4
    AVERAGE = 3
    POOR = 2
    FAILED = 1

@dataclass
class QATestCase:
    """Quality assurance test case"""
    test_id: str
    name: str
    description: str
    accent: str
    scenario: str
    expected_behavior: str
    test_audio: Optional[str] = None
    background_noise: bool = False
    
@dataclass
class CallMetrics:
    """Comprehensive call metrics"""
    call_id: str
    session_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Audio quality metrics
    amd_accuracy: Optional[float] = None  # Answering machine detection
    dtmf_accuracy: Optional[float] = None  # Touch tone recognition
    noise_level: Optional[float] = None    # Background noise level
    
    # Conversation metrics
    turns_completed: int = 0
    successful_interruptions: int = 0
    failed_interruptions: int = 0
    escalations: int = 0
    
    # Latency metrics
    avg_response_time_ms: Optional[float] = None
    p95_response_time_ms: Optional[float] = None
    first_response_time_ms: Optional[float] = None
    
    # Business metrics
    call_resolution: str = "unknown"  # resolved, escalated, dropped
    customer_satisfaction: Optional[int] = None  # 1-5 rating
    containment_achieved: bool = False
    
    # Error tracking
    error_count: int = 0
    error_types: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.error_types is None:
            self.error_types = []

class ObservabilityCollector:
    """Collect and aggregate observability metrics"""
    
    def __init__(self):
        self.call_metrics: Dict[str, CallMetrics] = {}
        self.daily_stats: Dict[str, Dict] = {}
        self.alert_rules = {
            "high_latency_threshold_ms": 500,
            "low_amd_accuracy_threshold": 0.85,
            "high_error_rate_threshold": 0.05,
            "low_containment_threshold": 0.70
        }
        self.logger = logging.getLogger(__name__)
    
    def start_call_tracking(self, call_id: str, session_id: str) -> CallMetrics:
        """Start tracking a new call"""
        metrics = CallMetrics(
            call_id=call_id,
            session_id=session_id,
            start_time=datetime.now()
        )
        
        self.call_metrics[call_id] = metrics
        self.logger.info(f"Call tracking started | call_id={call_id} | session_id={session_id}")
        return metrics
    
    def update_amd_result(self, call_id: str, is_human: bool, confidence: float):
        """Update answering machine detection result"""
        if call_id in self.call_metrics:
            self.call_metrics[call_id].amd_accuracy = confidence
            self.logger.debug(f"AMD result | call_id={call_id} | human={is_human} | confidence={confidence}")
    
    def update_turn_metrics(self, call_id: str, response_time_ms: float, successful: bool):
        """Update turn-level metrics"""
        if call_id not in self.call_metrics:
            return
        
        metrics = self.call_metrics[call_id]
        metrics.turns_completed += 1
        
        # Update response times
        if metrics.avg_response_time_ms is None:
            metrics.avg_response_time_ms = response_time_ms
            metrics.first_response_time_ms = response_time_ms
        else:
            # Running average
            total_time = metrics.avg_response_time_ms * (metrics.turns_completed - 1)
            metrics.avg_response_time_ms = (total_time + response_time_ms) / metrics.turns_completed
        
        self.logger.debug(f"Turn metrics | call_id={call_id} | response_time={response_time_ms:.1f}ms | successful={successful}")
    
    def record_interruption(self, call_id: str, successful: bool):
        """Record barge-in/interruption attempt"""
        if call_id in self.call_metrics:
            if successful:
                self.call_metrics[call_id].successful_interruptions += 1
            else:
                self.call_metrics[call_id].failed_interruptions += 1
    
    def record_escalation(self, call_id: str, reason: str):
        """Record call escalation to human"""
        if call_id in self.call_metrics:
            self.call_metrics[call_id].escalations += 1
            self.logger.info(f"Call escalated | call_id={call_id} | reason={reason}")
    
    def end_call_tracking(self, call_id: str, resolution: str, satisfaction: Optional[int] = None):
        """End call tracking and finalize metrics"""
        if call_id not in self.call_metrics:
            return
        
        metrics = self.call_metrics[call_id]
        metrics.end_time = datetime.now()
        metrics.call_resolution = resolution
        metrics.customer_satisfaction = satisfaction
        metrics.containment_achieved = resolution == "resolved"
        
        # Check for alerts
        self._check_call_alerts(metrics)
        
        # Update daily stats
        self._update_daily_stats(metrics)
        
        self.logger.info(f"Call tracking ended | call_id={call_id} | resolution={resolution} | satisfaction={satisfaction}")
    
    def _check_call_alerts(self, metrics: CallMetrics):
        """Check if call metrics trigger any alerts"""
        alerts = []
        
        # High latency alert
        if metrics.avg_response_time_ms and metrics.avg_response_time_ms > self.alert_rules["high_latency_threshold_ms"]:
            alerts.append(f"High latency: {metrics.avg_response_time_ms:.1f}ms")
        
        # Low AMD accuracy
        if metrics.amd_accuracy and metrics.amd_accuracy < self.alert_rules["low_amd_accuracy_threshold"]:
            alerts.append(f"Low AMD accuracy: {metrics.amd_accuracy:.3f}")
        
        # High error rate
        if metrics.turns_completed > 0:
            error_rate = metrics.error_count / metrics.turns_completed
            if error_rate > self.alert_rules["high_error_rate_threshold"]:
                alerts.append(f"High error rate: {error_rate:.3f}")
        
        for alert in alerts:
            self.logger.warning(f"ALERT | call_id={metrics.call_id} | {alert}")
    
    def _update_daily_stats(self, metrics: CallMetrics):
        """Update daily aggregated statistics"""
        date_key = metrics.start_time.strftime("%Y-%m-%d")
        
        if date_key not in self.daily_stats:
            self.daily_stats[date_key] = {
                "total_calls": 0,
                "resolved_calls": 0,
                "escalated_calls": 0,
                "total_turns": 0,
                "total_response_time": 0,
                "satisfaction_scores": [],
                "amd_accuracies": []
            }
        
        stats = self.daily_stats[date_key]
        stats["total_calls"] += 1
        stats["total_turns"] += metrics.turns_completed
        
        if metrics.call_resolution == "resolved":
            stats["resolved_calls"] += 1
        elif metrics.escalations > 0:
            stats["escalated_calls"] += 1
        
        if metrics.avg_response_time_ms:
            stats["total_response_time"] += metrics.avg_response_time_ms
        
        if metrics.customer_satisfaction:
            stats["satisfaction_scores"].append(metrics.customer_satisfaction)
        
        if metrics.amd_accuracy:
            stats["amd_accuracies"].append(metrics.amd_accuracy)
    
    def get_daily_summary(self, date: Optional[str] = None) -> Dict[str, Any]:
        """Get daily performance summary"""
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        if date not in self.daily_stats:
            return {"error": f"No data for date {date}"}
        
        stats = self.daily_stats[date]
        
        # Calculate derived metrics
        containment_rate = stats["resolved_calls"] / stats["total_calls"] if stats["total_calls"] > 0 else 0
        escalation_rate = stats["escalated_calls"] / stats["total_calls"] if stats["total_calls"] > 0 else 0
        avg_response_time = stats["total_response_time"] / stats["total_calls"] if stats["total_calls"] > 0 else 0
        avg_satisfaction = statistics.mean(stats["satisfaction_scores"]) if stats["satisfaction_scores"] else 0
        avg_amd_accuracy = statistics.mean(stats["amd_accuracies"]) if stats["amd_accuracies"] else 0
        
        return {
            "date": date,
            "total_calls": stats["total_calls"],
            "containment_rate": containment_rate,
            "escalation_rate": escalation_rate,
            "avg_response_time_ms": avg_response_time,
            "avg_satisfaction": avg_satisfaction,
            "avg_amd_accuracy": avg_amd_accuracy,
            "total_turns": stats["total_turns"]
        }

class QATestSuite:
    """Quality assurance test suite for voice system"""
    
    def __init__(self):
        self.test_cases = self._load_test_cases()
        self.test_results: List[Dict] = []
        self.logger = logging.getLogger(__name__)
    
    def _load_test_cases(self) -> List[QATestCase]:
        """Load predefined test cases"""
        return [
            # Accent variations
            QATestCase(
                test_id="accent_tx_01",
                name="Texas Accent - Account Inquiry",
                description="Test comprehension of strong Texas accent",
                accent="texas",
                scenario="account_inquiry",
                expected_behavior="Understand request and provide account information"
            ),
            QATestCase(
                test_id="accent_nyc_01", 
                name="NYC Accent - Complaint",
                description="Test handling of New York City accent with complaint",
                accent="nyc",
                scenario="complaint",
                expected_behavior="Show empathy and escalate appropriately"
            ),
            QATestCase(
                test_id="accent_southern_01",
                name="Southern Accent - Support Request",
                description="Test Southern accent with background noise",
                accent="southern",
                scenario="support_request",
                expected_behavior="Request clarification if needed, provide support",
                background_noise=True
            ),
            QATestCase(
                test_id="accent_midwest_01",
                name="Midwest Accent - Sales Inquiry",
                description="Test clear Midwest accent with sales question",
                accent="midwest",
                scenario="sales_inquiry",
                expected_behavior="Provide product information and capture interest"
            ),
            QATestCase(
                test_id="accent_indian_01",
                name="Indian English - Technical Support",
                description="Test Indian-accented English with technical terms",
                accent="indian",
                scenario="technical_support",
                expected_behavior="Understand technical vocabulary and provide solutions"
            ),
            QATestCase(
                test_id="accent_spanish_01",
                name="Spanish-accented English - Billing",
                description="Test Spanish-accented English with billing questions",
                accent="spanish_english",
                scenario="billing_inquiry",
                expected_behavior="Understand inquiry and explain billing clearly"
            ),
            
            # Edge case scenarios
            QATestCase(
                test_id="edge_barge_in_01",
                name="Mid-sentence Interruption",
                description="Test interruption handling during AI response",
                accent="neutral",
                scenario="interruption_test",
                expected_behavior="Stop speaking immediately and listen to customer"
            ),
            QATestCase(
                test_id="edge_dtmf_01",
                name="Mid-speech DTMF Tones",
                description="Test handling of touch tones during conversation",
                accent="neutral", 
                scenario="dtmf_interrupt",
                expected_behavior="Recognize DTMF and route appropriately"
            ),
            QATestCase(
                test_id="edge_mumbling_01",
                name="Unclear Speech",
                description="Test handling of mumbled or unclear speech",
                accent="unclear",
                scenario="unclear_speech",
                expected_behavior="Politely request clarification"
            ),
            QATestCase(
                test_id="edge_long_pause_01",
                name="Extended Silence",
                description="Test behavior during long customer pauses",
                accent="neutral",
                scenario="long_pause",
                expected_behavior="Gentle prompt after 3-5 seconds"
            ),
            QATestCase(
                test_id="edge_code_switch_01",
                name="Language Code-switching",
                description="Test Spanish to English language switching",
                accent="bilingual",
                scenario="language_switch",
                expected_behavior="Detect language change and adapt accordingly"
            )
        ]
    
    async def run_test_case(self, test_case: QATestCase, voice_system) -> Dict[str, Any]:
        """Run a single test case against the voice system"""
        
        start_time = time.time()
        self.logger.info(f"Running test case: {test_case.test_id} - {test_case.name}")
        
        # Simulate test execution
        # In production: this would make actual calls to the voice system
        await asyncio.sleep(random.uniform(0.5, 2.0))  # Simulate test duration
        
        # Simulate test results
        success_rate = random.uniform(0.7, 1.0)  # Random success for demo
        response_time = random.uniform(200, 800)  # Random response time
        
        result = {
            "test_id": test_case.test_id,
            "test_name": test_case.name,
            "accent": test_case.accent,
            "scenario": test_case.scenario,
            "success": success_rate > 0.8,
            "success_rate": success_rate,
            "response_time_ms": response_time,
            "expected_behavior": test_case.expected_behavior,
            "actual_behavior": "Responded appropriately" if success_rate > 0.8 else "Failed to understand",
            "execution_time": time.time() - start_time,
            "background_noise": test_case.background_noise,
            "timestamp": datetime.now().isoformat()
        }
        
        self.test_results.append(result)
        
        status = "‚úÖ PASS" if result["success"] else "‚ùå FAIL"
        self.logger.info(f"Test {test_case.test_id}: {status} | {response_time:.1f}ms | {success_rate:.1%}")
        
        return result
    
    async def run_full_suite(self, voice_system=None) -> Dict[str, Any]:
        """Run the complete QA test suite"""
        
        self.logger.info(f"Starting QA test suite | {len(self.test_cases)} test cases")
        suite_start = time.time()
        
        # Run all test cases
        tasks = [self.run_test_case(test_case, voice_system) for test_case in self.test_cases]
        results = await asyncio.gather(*tasks)
        
        # Calculate suite metrics
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r["success"])
        failed_tests = total_tests - passed_tests
        
        avg_response_time = statistics.mean([r["response_time_ms"] for r in results])
        p95_response_time = statistics.quantiles([r["response_time_ms"] for r in results], n=20)[18] if len(results) >= 20 else 0
        
        # Group results by category
        accent_results = {}
        scenario_results = {}
        
        for result in results:
            # Group by accent
            accent = result["accent"]
            if accent not in accent_results:
                accent_results[accent] = {"total": 0, "passed": 0}
            accent_results[accent]["total"] += 1
            if result["success"]:
                accent_results[accent]["passed"] += 1
            
            # Group by scenario
            scenario = result["scenario"]
            if scenario not in scenario_results:
                scenario_results[scenario] = {"total": 0, "passed": 0}
            scenario_results[scenario]["total"] += 1
            if result["success"]:
                scenario_results[scenario]["passed"] += 1
        
        suite_summary = {
            "execution_time": time.time() - suite_start,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "pass_rate": passed_tests / total_tests,
            "avg_response_time_ms": avg_response_time,
            "p95_response_time_ms": p95_response_time,
            "accent_breakdown": {
                accent: {"pass_rate": data["passed"] / data["total"], "count": data["total"]}
                for accent, data in accent_results.items()
            },
            "scenario_breakdown": {
                scenario: {"pass_rate": data["passed"] / data["total"], "count": data["total"]}
                for scenario, data in scenario_results.items()
            },
            "failed_tests": [r for r in results if not r["success"]],
            "timestamp": datetime.now().isoformat()
        }
        
        self.logger.info(f"QA Suite Complete | Pass Rate: {suite_summary['pass_rate']:.1%} | Avg Response: {avg_response_time:.1f}ms")
        
        return suite_summary
    
    def generate_qa_report(self, results: Dict[str, Any]) -> str:
        """Generate human-readable QA report"""
        
        report = f"""
üîç GhostVoiceGPT QA Test Report
{'=' * 50}

üìä Executive Summary:
  ‚Ä¢ Total Tests: {results['total_tests']}
  ‚Ä¢ Pass Rate: {results['pass_rate']:.1%}
  ‚Ä¢ Average Response Time: {results['avg_response_time_ms']:.1f}ms
  ‚Ä¢ P95 Response Time: {results['p95_response_time_ms']:.1f}ms
  ‚Ä¢ Execution Time: {results['execution_time']:.1f}s

üìà Performance by Accent:
"""
        
        for accent, data in results['accent_breakdown'].items():
            status = "‚úÖ" if data['pass_rate'] >= 0.9 else "‚ö†Ô∏è" if data['pass_rate'] >= 0.7 else "‚ùå"
            report += f"  {status} {accent:20} | Pass Rate: {data['pass_rate']:.1%} | Tests: {data['count']}\n"
        
        report += f"""
üìã Performance by Scenario:
"""
        
        for scenario, data in results['scenario_breakdown'].items():
            status = "‚úÖ" if data['pass_rate'] >= 0.9 else "‚ö†Ô∏è" if data['pass_rate'] >= 0.7 else "‚ùå"
            report += f"  {status} {scenario:20} | Pass Rate: {data['pass_rate']:.1%} | Tests: {data['count']}\n"
        
        if results['failed_tests']:
            report += f"""
‚ùå Failed Tests ({len(results['failed_tests'])}):
"""
            for test in results['failed_tests']:
                report += f"  ‚Ä¢ {test['test_id']}: {test['test_name']} | {test['accent']} | {test['response_time_ms']:.1f}ms\n"
        
        report += f"""
üéØ Recommendations:
"""
        
        if results['pass_rate'] >= 0.95:
            report += "  ‚úÖ Excellent performance! System is production-ready.\n"
        elif results['pass_rate'] >= 0.85:
            report += "  ‚ö†Ô∏è  Good performance. Consider tuning failed test scenarios.\n"
        else:
            report += "  ‚ùå Performance needs improvement before production deployment.\n"
        
        if results['avg_response_time_ms'] > 500:
            report += "  ‚ö†Ô∏è  Response times are high. Consider optimizing latency.\n"
        
        return report

# Usage demonstration
async def demo_observability_system():
    """Demo the complete observability and QA system"""
    print("üìä Production Observability & QA Demo")
    print("=" * 50)
    
    # Initialize systems
    observability = ObservabilityCollector()
    qa_suite = QATestSuite()
    
    # Demo 1: Call tracking
    print("\n1Ô∏è‚É£ Call Tracking Demo")
    print("-" * 25)
    
    call_id = "demo_call_123"
    session_id = "demo_session_456"
    
    # Start call tracking
    metrics = observability.start_call_tracking(call_id, session_id)
    
    # Simulate call events
    observability.update_amd_result(call_id, True, 0.95)
    observability.update_turn_metrics(call_id, 250.0, True)
    observability.update_turn_metrics(call_id, 180.0, True)
    observability.record_interruption(call_id, True)
    
    # End call
    observability.end_call_tracking(call_id, "resolved", 5)
    
    # Demo 2: QA Test Suite
    print("\n2Ô∏è‚É£ QA Test Suite Demo")
    print("-" * 25)
    
    # Run a subset of tests for demo
    test_subset = qa_suite.test_cases[:5]  # First 5 tests
    
    print(f"Running {len(test_subset)} test cases...")
    
    suite_results = await qa_suite.run_full_suite()
    
    # Generate and display report
    print("\nüìã QA Test Report:")
    report = qa_suite.generate_qa_report(suite_results)
    print(report)
    
    # Demo 3: Daily summary
    print("\n3Ô∏è‚É£ Daily Performance Summary")
    print("-" * 35)
    
    daily_summary = observability.get_daily_summary()
    print("üìä Today's Performance:")
    for key, value in daily_summary.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    print("\n‚úÖ Observability system ready for production!")
    print("   - Real-time call tracking and metrics")
    print("   - Comprehensive QA test automation")
    print("   - Daily performance reporting")
    print("   - Automated alerting and monitoring")

if __name__ == "__main__":
    asyncio.run(demo_observability_system())